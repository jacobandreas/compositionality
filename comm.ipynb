{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import evals\n",
    "from util import Logger\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler as opt_sched\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCH = 256\n",
    "N_EMBED = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(var):\n",
    "    return var.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATask(object):\n",
    "    N_FIELDS = 2\n",
    "    N_VALUES = 4\n",
    "    \n",
    "    @classmethod\n",
    "    def sample(cls):\n",
    "        n_fields = cls.N_FIELDS\n",
    "        n_values = cls.N_VALUES\n",
    "        \n",
    "        tar_pairs = []\n",
    "        dis_pairs = []\n",
    "        common_field = np.random.randint(n_fields)\n",
    "        common_value = np.random.randint(n_values)\n",
    "        if np.random.random() < 0.75:\n",
    "            tar_pairs.append((common_field, common_value))\n",
    "            dis_pairs.append((common_field, common_value))\n",
    "        \n",
    "        assert n_fields == 2\n",
    "        other_field = 1 - common_field\n",
    "        while True:\n",
    "            ov1, ov2 = np.random.randint(n_values, size=2)\n",
    "            if ov1 != ov2:\n",
    "                break\n",
    "        tar_pairs.append((other_field, ov1))\n",
    "        dis_pairs.append((other_field, ov2))\n",
    "        \n",
    "        return QATask.create_with(tar_pairs, dis_pairs)\n",
    "                      \n",
    "    @classmethod\n",
    "    def create_with(cls, tar_pairs, dis_pairs):\n",
    "        n_fields = cls.N_FIELDS\n",
    "        n_values = cls.N_VALUES\n",
    "        \n",
    "        tar_feats = np.zeros((n_fields, n_values))\n",
    "        dis_feats = np.zeros((n_fields, n_values))\n",
    "        \n",
    "        for pair in tar_pairs:\n",
    "            tar_feats[pair] = 1\n",
    "        for pair in dis_pairs:\n",
    "            dis_feats[pair] = 1\n",
    "            \n",
    "        assert n_fields == 2\n",
    "        pretty = [''.join('%d:%d' % pair for pair in sorted(pairs)) for pairs in [tar_pairs, dis_pairs]]\n",
    "        return QATask(tar_feats.ravel(), dis_feats.ravel(), pretty)\n",
    "    \n",
    "    def __init__(self, tar_feats, dis_feats, pretty):\n",
    "        self.tar_feats = tar_feats\n",
    "        self.dis_feats = dis_feats\n",
    "        self.pretty = pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 256\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, start_sym, stop_sym):\n",
    "        hid = N_HIDDEN\n",
    "        super().__init__()\n",
    "        self._vocab = vocab\n",
    "        self._start_id = vocab[start_sym]\n",
    "        self._stop_id = vocab[stop_sym]\n",
    "\n",
    "        #self._embed = nn.Linear(len(vocab), hid)\n",
    "        #self._rnn = nn.GRU(input_size=hid, hidden_size=hid, num_layers=1)\n",
    "        self._rnn = nn.GRU(input_size=len(vocab), hidden_size=hid, num_layers=1)\n",
    "        self._predict = nn.Linear(hid, len(vocab))\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, state, inp):\n",
    "        #emb = self._embed(inp)\n",
    "        emb = inp\n",
    "        rep, enc = self._rnn(emb, state)\n",
    "        logits = self._predict(rep)\n",
    "        return enc, logits\n",
    "\n",
    "    def decode(self, init_state, max_len, sample=False):\n",
    "        n_stack, n_batch, _ = init_state.shape\n",
    "        out = [[self._start_id] for _ in range(n_batch)]\n",
    "        tok_inp = [self._start_id for _ in range(n_batch)]\n",
    "        done = [False for _ in range(n_batch)]\n",
    "        state = init_state\n",
    "        for _ in range(max_len):\n",
    "            hot_inp = np.zeros((1, n_batch, len(self._vocab)))\n",
    "            for i, t in enumerate(tok_inp):\n",
    "                hot_inp[0, i, t] = 1\n",
    "            hot_inp = Variable(torch.FloatTensor(hot_inp))\n",
    "            if init_state.is_cuda:\n",
    "                hot_inp = hot_inp.cuda()\n",
    "            new_state, label_logits = self(state, hot_inp)\n",
    "            label_logits = label_logits.squeeze(0)\n",
    "            label_probs = unwrap(self._softmax(label_logits))\n",
    "            new_tok_inp = []\n",
    "            for i, row in enumerate(label_probs):\n",
    "                if sample:\n",
    "                    tok = np.random.choice(row.size, p=row)\n",
    "                else:\n",
    "                    tok = row.argmax()\n",
    "                new_tok_inp.append(tok)\n",
    "                if not done[i]:\n",
    "                    out[i].append(tok)\n",
    "                done[i] = done[i] or tok == self._stop_id\n",
    "            state = new_state\n",
    "            tok_inp = new_tok_inp\n",
    "            if all(done):\n",
    "                break\n",
    "        return out\n",
    "\n",
    "class SModel(nn.Module):\n",
    "    def __init__(self, n_vocab):\n",
    "        super().__init__()\n",
    "        self._rep = nn.Linear(QATask.N_FIELDS * QATask.N_VALUES, N_HIDDEN)\n",
    "        self.vocab = {'<s>': 0, '</s>': 1}\n",
    "        self.vocab.update({chr(ord('a') + i): i+2 for i in range(n_vocab)})\n",
    "        self._decoder = Decoder(self.vocab, '<s>', '</s>')\n",
    "        self._nll = nn.CrossEntropyLoss(reduce=False)\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "        self._log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def unencode(self, t):\n",
    "        for k, v in self.vocab.items():\n",
    "            if v == t:\n",
    "                return k\n",
    "        \n",
    "    def sample(self, obs, max_len, max=False):\n",
    "        rep = self._rep(obs).unsqueeze(0)\n",
    "        dec = self._decoder.decode(rep, max_len, sample=not max)\n",
    "        return dec\n",
    "    \n",
    "    def forward(self, obs, msg, msg_tgts, mask):\n",
    "        rep = self._rep(obs).unsqueeze(0)\n",
    "        _, logits = self._decoder(rep, msg)\n",
    "        time, batch, vocab = logits.shape\n",
    "        logits = logits.view(time * batch, vocab)\n",
    "        msg_tgts = msg_tgts.view(time * batch)\n",
    "        nll = self._nll(logits, msg_tgts).view(time, batch) * mask\n",
    "        ent = -(self._softmax(logits) * self._log_softmax(logits)).sum(dim=1).view(time, batch) * mask\n",
    "        \n",
    "        return nll.sum(dim=0), ent.sum(dim=0)\n",
    "        \n",
    "class LModel(nn.Module):\n",
    "    def __init__(self, s_model):\n",
    "        super().__init__()\n",
    "        n_vocab = len(s_model.vocab)\n",
    "        self._rep = nn.Linear(QATask.N_FIELDS * QATask.N_VALUES, N_HIDDEN)\n",
    "        #self._rnn = nn.GRU(input_size=n_vocab, hidden_size=N_HIDDEN, num_layers=1)\n",
    "        self._proj = nn.Linear(n_vocab, N_HIDDEN)\n",
    "        self._nll = nn.CrossEntropyLoss(reduce=False)\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, tar_feats, dis_feats, msg_feats):\n",
    "        rep_tar = self._rep(tar_feats)\n",
    "        rep_dis = self._rep(dis_feats)\n",
    "        #_, pred = self._rnn(msg_feats)\n",
    "        pred = self._proj(msg_feats.sum(dim=0))\n",
    "        pred = pred.squeeze(0)\n",
    "        \n",
    "        scores_tar = (pred * rep_tar).sum(1)\n",
    "        scores_dis = (pred * rep_dis).sum(1)\n",
    "        scores = torch.stack((scores_tar, scores_dis), dim=1)\n",
    "        \n",
    "        indices = torch.LongTensor(scores.shape[0]).zero_()\n",
    "        nll = self._nll(scores, Variable(indices))\n",
    "        \n",
    "        probs = unwrap(self._softmax(scores))\n",
    "        preds = np.asarray([np.random.choice(2, p=row) for row in probs])\n",
    "        return nll, (preds == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0:                  1:                  2:                  3:                  \n",
      "0: kkkk             0,0:                0,1: kkkkkkk        0,2: kkkkkkk        0,3:                \n",
      "1:                  1,0:                1,1:                1,2:                1,3:                \n",
      "2:                  2,0:                2,1:                2,2:                2,3:                \n",
      "3:                  3,0:                3,1:                3,2:                3,3:                \n",
      "-0.764 0.515 0.000\n",
      "\n",
      "                    0:                  1:   ipppppp        2:   kkkkkkk        3:   ppppppp        \n",
      "0: kkkpppp          0,0: eeepfff        0,1: iiiiiii        0,2: kkkkkkk        0,3: kpppppp        \n",
      "1:                  1,0:                1,1:                1,2:                1,3:                \n",
      "2:                  2,0:                2,1: g              2,2: kkkkkkk        2,3:                \n",
      "3: ppppppp          3,0: ppppppp        3,1: ppppppp        3,2: kkkkkpp        3,3: ppppppp        \n",
      "0.284 0.637 0.214\n",
      "\n",
      "                    0:   fffffff        1:   iiiiiii        2:   kkkkkkp        3:   ppppppp        \n",
      "0: iiiiiii          0,0: fffffff        0,1: iiiiiii        0,2: kkkkkii        0,3: ppppppp        \n",
      "1:                  1,0:                1,1: llg            1,2:                1,3:                \n",
      "2: gg               2,0: ff             2,1: gggggg         2,2: kkkk           2,3:                \n",
      "3: ppppppp          3,0: ppppfff        3,1: iiiiiii        3,2: kkkkppp        3,3: ppppppp        \n",
      "-0.258 0.795 0.271\n",
      "\n",
      "                    0:   fffffff        1:   gggggii        2:   kkkkk          3:   ppppppp        \n",
      "0: kkieeff          0,0: fffffff        0,1: iiiiiii        0,2: kkkkkkk        0,3: kpppppp        \n",
      "1:                  1,0: ff             1,1: lllllll        1,2: k              1,3:                \n",
      "2: gg               2,0: ff             2,1: ggggggg        2,2: kkk            2,3:                \n",
      "3: ppppppp          3,0: pppffff        3,1: hhhhhhh        3,2: kkooppp        3,3: ppppppp        \n",
      "-0.613 0.851 0.313\n",
      "\n",
      "                    0:   fffffff        1:   iiiiiii        2:   kkooooo        3:   pppmmmm        \n",
      "0: kkieeee          0,0: fffffff        0,1: iiiiiii        0,2: kkkkkkk        0,3: keepppp        \n",
      "1: nmmmmmm          1,0: ffmmmmm        1,1: lllllll        1,2: noooomm        1,3: mmmmmmm        \n",
      "2: gggg             2,0: fff            2,1: ggggggg        2,2: kkkon          2,3: g              \n",
      "3: ooopppp          3,0: fffdddd        3,1: hhhiiii        3,2: ooooooo        3,3: ppppppp        \n",
      "-0.750 0.914 0.507\n",
      "\n",
      "                    0:   fffmmmm        1:   iiiiiii        2:   kkkkoon        3:   ppppppp        \n",
      "0: eeeeeee          0,0: ffffeee        0,1: iiiiiii        0,2: kkkkkkk        0,3: eeeeepp        \n",
      "1: nmmmmmm          1,0: fmmmmmm        1,1: lllllll        1,2: nnnnnnn        1,3: mmmmmmm        \n",
      "2: ggggggg          2,0: ffffgg         2,1: ggggiii        2,2: kkkkkgg        2,3: gggg           \n",
      "3: ooppppp          3,0: ddddmmm        3,1: hhhhhhh        3,2: ooooooo        3,3: ppppppp        \n",
      "-1.051 0.951 0.524\n",
      "\n",
      "                    0:   ffddddd        1:   iiiiiii        2:   koooooc        3:   ppppppp        \n",
      "0: eeeeeee          0,0: fffeeee        0,1: iiiiiii        0,2: kkkkeee        0,3: eeeeepp        \n",
      "1: nllllll          1,0: fmmmlll        1,1: lllllhh        1,2: nnnoool        1,3: mmmmmll        \n",
      "2: ggggggg          2,0: ffffgg         2,1: gggiiii        2,2: kkkkkkk        2,3: gg             \n",
      "3: occccdd          3,0: ddddddd        3,1: hhhhhhh        3,2: ooooocc        3,3: ppppppp        \n",
      "-1.047 0.953 0.501\n",
      "\n",
      "                    0:   ffddddd        1:   iiiiiii        2:   kkooood        3:   ppp            \n",
      "0: eeeeeee          0,0: fffffdd        0,1: iiiiiii        0,2: kkkkkee        0,3: eeppppp        \n",
      "1: lllllll          1,0: ffmmmll        1,1: llllllh        1,2: nnnnnll        1,3: mm             \n",
      "2: ggggg            2,0: ffffg          2,1: gggggii        2,2: kaaaaaa        2,3: g              \n",
      "3: opppppp          3,0: ddddddd        3,1: hhhhhhh        3,2: oooooop        3,3: ppppppp        \n",
      "-1.168 0.958 0.578\n",
      "\n",
      "                    0:   ffddddd        1:   iiiiiii        2:   kkoooaa        3:   ppp            \n",
      "0: eeeeeep          0,0: ffffeed        0,1: iiiiiii        0,2: kkkkkep        0,3: eeppppp        \n",
      "1: mllllll          1,0: fmmmlll        1,1: llllllh        1,2: nnnnnnl        1,3: mmm            \n",
      "2: gggg             2,0: fffga          2,1: ggggggi        2,2: kkaaaaa        2,3: gg             \n",
      "3: ohddddd          3,0: ddddddd        3,1: hhhhhhh        3,2: ooooodd        3,3: ppppppp        \n",
      "-1.049 0.971 0.568\n",
      "\n",
      "                    0:   ffffddd        1:   iiiiiii        2:   koooaaa        3:   ppp            \n",
      "0: eeeeeee          0,0: ffffffe        0,1: iiiiiii        0,2: kkkkkke        0,3: eeeppp         \n",
      "1: mmmmlll          1,0: fmmmlll        1,1: lllllhh        1,2: nnnnnnn        1,3: mmm            \n",
      "2: gggg             2,0: ffffg          2,1: gggiiii        2,2: kkkaaa         2,3: g              \n",
      "3: ohppcdd          3,0: ddddddd        3,1: hhhhhhh        3,2: ooooooo        3,3: ppppppp        \n",
      "-1.144 0.974 0.572\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-23de24ceec36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mdis_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdis_feats\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mmsgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mmsg_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_BATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-219d001dd268>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, obs, max_len, max)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-219d001dd268>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, init_state, max_len, sample)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_BATCH = 64\n",
    "MAX_LEN = 8\n",
    "VOCAB_SIZE = 16\n",
    "COMM_COST = .01\n",
    "START_COST = .05\n",
    "ENT_BONUS = .01\n",
    "\n",
    "def str_dist(x, y):\n",
    "    c1 = Counter(x)\n",
    "    c2 = Counter(y)\n",
    "    s1 = sum(c1.values())\n",
    "    s2 = sum(c2.values())\n",
    "    c1 = Counter({k: v / s1 for k, v in c1.items()})\n",
    "    c2 = Counter({k: v / s2 for k, v in c2.items()})\n",
    "    return sum(abs(a) for a in (c1-c2).values())\n",
    "\n",
    "def validate(s_model, l_model):\n",
    "    def get_msg(task):\n",
    "        msg, = s_model.sample(Variable(torch.FloatTensor([task.tar_feats])), MAX_LEN, max=True)\n",
    "        return ''.join(s_model.unencode(t) for t in msg[1:-1])\n",
    "    \n",
    "    prim = []\n",
    "    comp = []\n",
    "    \n",
    "    line = ['']\n",
    "    for v2 in range(QATask.N_VALUES):\n",
    "        task = QATask.create_with([(1, v2)], [])\n",
    "        msg = get_msg(task)\n",
    "        prim.append(('1:%d' % v2, msg))\n",
    "        line.append('%s:   ' % v2 + msg)\n",
    "    print(''.join('%-20s' % l for l in line))\n",
    "        \n",
    "    for v1 in range(QATask.N_VALUES):\n",
    "        line = []\n",
    "        task = QATask.create_with([(0, v1)], [])\n",
    "        msg = get_msg(task)\n",
    "        prim.append(('0:%d' % v1, msg))\n",
    "        line.append('%s: ' % v1 + msg)\n",
    "        for v2 in range(QATask.N_VALUES):\n",
    "            task = QATask.create_with([(0, v1), (1, v2)], [])\n",
    "            msg = get_msg(task)\n",
    "            comp.append((('0:%d' % v1, '1:%d' % v2), msg))\n",
    "            line.append('%s,%s: ' % (v1, v2) + msg)\n",
    "        print(''.join('%-20s' % l for l in line))\n",
    "        \n",
    "    e_prim, r_prim = zip(*prim)\n",
    "    e_comp, r_comp = zip(*comp)\n",
    "    return np.mean(evals.comp_eval(r_prim, e_prim, r_comp, e_comp, lambda x, y: x + y, str_dist))\n",
    "\n",
    "for restart in range(10):\n",
    "    s_model = SModel(VOCAB_SIZE)\n",
    "    l_model = LModel(s_model)\n",
    "\n",
    "    s_opt = optim.Adam(s_model.parameters(), lr=1e-3)\n",
    "    l_opt = optim.Adam(l_model.parameters(), lr=1e-3)\n",
    "\n",
    "    for i_epoch in range(10):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        #print('L' if i_epoch % 2 == 0 else 'S')\n",
    "        for i in range(50):\n",
    "            tasks = [QATask.sample() for _ in range(N_BATCH)]\n",
    "            tar_feats = Variable(torch.FloatTensor([t.tar_feats for t in tasks]))\n",
    "            dis_feats = Variable(torch.FloatTensor([t.dis_feats for t in tasks]))\n",
    "\n",
    "            msgs = s_model.sample(tar_feats, MAX_LEN)\n",
    "\n",
    "            msg_feats = np.zeros((MAX_LEN, N_BATCH, len(s_model.vocab)))\n",
    "            msg_targets = np.zeros((MAX_LEN, N_BATCH))\n",
    "            msg_mask = np.zeros((MAX_LEN, N_BATCH))\n",
    "            for i_msg, msg in enumerate(msgs):\n",
    "                for t in range(len(msg)-1):\n",
    "                    msg_feats[t, i_msg, msg[t]] = 1\n",
    "                    msg_targets[t, i_msg] = msg[t+1]\n",
    "                    msg_mask[t, i_msg] = 1\n",
    "            msg_feats = Variable(torch.FloatTensor(msg_feats))\n",
    "            msg_targets = Variable(torch.LongTensor(msg_targets))\n",
    "            msg_mask = Variable(torch.FloatTensor(msg_mask))\n",
    "\n",
    "            l_nll, l_acc = l_model(tar_feats, dis_feats, msg_feats)\n",
    "            l_loss = l_nll.mean()\n",
    "\n",
    "            l_opt.zero_grad()\n",
    "            l_loss.backward()\n",
    "            nn.utils.clip_grad_norm(l_model.parameters(), 1) \n",
    "            l_opt.step()\n",
    "            loss += unwrap(l_nll).mean()\n",
    "\n",
    "            dl_nll = l_nll.detach()\n",
    "            reward = -(\n",
    "                dl_nll - dl_nll.mean() \n",
    "                + COMM_COST * Variable(torch.FloatTensor([len(msg)-1 for msg in msgs]))\n",
    "                + START_COST * Variable(torch.FloatTensor([sum(1 for t in msg[1:] if t == 0) for msg in msgs]))\n",
    "                )\n",
    "            s_nll, s_ent = s_model(tar_feats, msg_feats, msg_targets, msg_mask)\n",
    "            s_surr = s_nll * reward - ENT_BONUS * s_ent\n",
    "            s_surr_loss = s_surr.mean()\n",
    "            s_opt.zero_grad()\n",
    "            s_surr_loss.backward()\n",
    "            nn.utils.clip_grad_norm(s_model.parameters(), 1) \n",
    "            s_opt.step()\n",
    "            loss += unwrap(s_surr).mean()\n",
    "\n",
    "            acc += l_acc.mean()\n",
    "        comp = validate(s_model, l_model)\n",
    "        print('%0.3f %0.3f %0.3f' % (loss / 50, acc / 50, comp))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
