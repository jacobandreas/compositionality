{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from cls2_data.data import Dataset\n",
    "import evals\n",
    "from util import Logger\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import sexpdata\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler as opt_sched\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EMBED = 128\n",
    "N_HIDDEN = 256\n",
    "N_BATCH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(var):\n",
    "    return var.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self._conv_part = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self._fc_part = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self._pred_part = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self._loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        n_batch, n_ex, c, w, h = batch.feats_in.shape\n",
    "        conv_in = self._conv_part(batch.feats_in.view(n_batch * n_ex, c, w, h))\n",
    "        fc_in = self._fc_part(conv_in.view(n_batch * n_ex, 16*5*5))\n",
    "        #fc_in += 0 * Variable(torch.randn(fc_in.shape))\n",
    "        predictor = self._pred_part(fc_in.view(n_batch, n_ex, 64).sum(dim=1))\n",
    "        #predictor += 0.0 * Variable(torch.randn(predictor.shape))\n",
    "        \n",
    "        conv_out = self._conv_part(batch.feats_out)\n",
    "        rep_out = self._fc_part(conv_out.view(n_batch, 16*5*5))\n",
    "        #print(predictor[0])\n",
    "        #print(rep_out[0])\n",
    "        \n",
    "        score = (predictor * rep_out).sum(dim=1)\n",
    "        #print(score)\n",
    "        labels = (score > 0).float()\n",
    "        loss = self._loss(score, batch.label_out)\n",
    "        \n",
    "        return loss, (labels == batch.label_out).float().mean(), labels, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-6ce7010246da>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6ce7010246da>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    EPOCH = 'epoch'\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def eval_isom_tree(reps, lfs):\n",
    "    return evals.isom(reps, lfs, evals.cos_dist, evals.tree_dist)\n",
    "\n",
    "def eval_isom_ext(reps, exts):\n",
    "    return evals.isom(reps, exts, evals.cos_dist, evals.l1_dist)\n",
    "\n",
    "def eval_hom(model, reps1, reps2):\n",
    "    #sim_rep = lambda x, y: ((x-y)**2).sum()\n",
    "    return evals.hom(reps1, reps2, None)\n",
    "\n",
    "\n",
    "def info(model):\n",
    "    \n",
    "\n",
    "EPOCH = 'epoch'\n",
    "TRN_LOSS = 'trn loss'\n",
    "TRN_ACC = 'trn acc'\n",
    "VAL_ACC = 'val acc'\n",
    "CVAL_ACC = 'cval acc'\n",
    "ISOM_TREE = 'isom (r-t)'\n",
    "ISOM_EXT = 'isom (r-e)'\n",
    "ISOM_CHK = 'isom (t-e)'\n",
    "HOM = 'hom'\n",
    "CHOM = 'c_hom'\n",
    "LOG_KEYS = [EPOCH, TRN_LOSS, TRN_ACC, VAL_ACC, CVAL_ACC, HOM,   CHOM]\n",
    "LOG_FMTS = ['d',   '.3f',    '.3f',   '.3f',   '.3f',    '.3f', '.3f']\n",
    "\n",
    "def validate(dataset, model, logger):\n",
    "    val_batch = dataset.get_val_batch()\n",
    "    _, val_acc, _, val_reps = model(val_batch)\n",
    "    val_acc, = unwrap(val_acc)\n",
    "    logger.update(VAL_ACC, val_acc)\n",
    "    \n",
    "    cval_batch = dataset.get_cval_batch()\n",
    "    _, cval_acc, _, cval_reps = model(cval_batch)\n",
    "    cval_acc, = unwrap(cval_acc)\n",
    "    logger.update(CVAL_ACC, cval_acc)\n",
    "    \n",
    "    prim_batch = dataset.get_prim_batch()\n",
    "    _, _, _, prim_reps = model(prim_batch)\n",
    "    \n",
    "    comp = evals.comp_eval(\n",
    "        prim_reps.data.numpy(), prim_batch.lf, \n",
    "        val_reps.data.numpy(), val_batch.lf, \n",
    "        lambda x, y: (x+y), \n",
    "        evals.cos_dist)\n",
    "    logger.update(HOM, np.mean(comp))\n",
    "    \n",
    "    ccomp = evals.comp_eval(\n",
    "        prim_reps.data.numpy(), prim_batch.lf,\n",
    "        cval_reps.data.numpy(), cval_batch.lf,\n",
    "        lambda x, y: (x+y),\n",
    "        evals.cos_dist)\n",
    "    logger.update(CHOM, np.mean(ccomp))\n",
    "    \n",
    "    return val_acc\n",
    "    \n",
    "def validate_hom(dataset, model, logger):\n",
    "    hom_batch, groups = dataset.get_hom_batch(N_MTRAIN)\n",
    "    _, _, hom_preds, hom_reps = model(hom_batch)\n",
    "    hom_reps = unwrap(hom_reps)\n",
    "    \n",
    "    named = [\n",
    "        dataset.name(hom_batch.mids[m], hom_batch.indices[m][1], unwrap(hom_preds[m, :]))\n",
    "        for m in range(hom_preds.shape[0])]\n",
    "    true_lfs, _ = zip(*named)\n",
    "    good_ids = [i for i in range(len(hom_batch.lfs)) if hom_batch.lfs[i] == true_lfs[i]]\n",
    "    all_ids = list(range(len(hom_batch.lfs)))\n",
    "    \n",
    "    for ids, key in ((good_ids, HOM), (all_ids, HOM2)):\n",
    "        good_groups = [g for g in groups if all(gg in ids for gg in g)]\n",
    "        parents = [hom_reps[p] for p, _, _ in good_groups]\n",
    "        children = [(hom_reps[c1] + hom_reps[c2]) for _, c1, c2 in good_groups]\n",
    "        logger.update(key, evals.hom(parents, children, evals.cos_dist))\n",
    "\n",
    "def train(dataset, model):\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    sched = opt_sched.ReduceLROnPlateau(opt, factor=0.5, verbose=True, mode='max')\n",
    "    logger = Logger(LOG_KEYS, LOG_FMTS, width=10)\n",
    "    logger.begin()\n",
    "    \n",
    "    val_acc = validate(dataset, model, logger)\n",
    "    #validate_hom(dataset, model, logger)\n",
    "    logger.print()\n",
    "    \n",
    "    for i in range(100):\n",
    "        trn_loss = 0\n",
    "        trn_acc = 0\n",
    "        for j in range(100):\n",
    "            batch = dataset.get_train_batch(N_BATCH)\n",
    "            loss, acc, _, _ = model(batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            trn_loss += unwrap(loss)[0]\n",
    "            trn_acc += unwrap(acc)[0]\n",
    "        trn_loss /= 100\n",
    "        trn_acc /= 100\n",
    "        \n",
    "        logger.update(EPOCH, i)\n",
    "        logger.update(TRN_LOSS, trn_loss)\n",
    "        logger.update(TRN_ACC, trn_acc)\n",
    "        val_acc = validate(dataset, model, logger)\n",
    "        #validate_hom(dataset, model, logger)\n",
    "        sched.step(val_acc)\n",
    "        logger.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "model = Model(dataset)\n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
