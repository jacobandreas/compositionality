{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rep_data.data import Dataset\n",
    "import evals\n",
    "from util import Logger\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler as opt_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCH = 256\n",
    "N_EMBED = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(var):\n",
    "    return var.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self._emb = nn.Embedding(dataset.n_vocab, N_EMBED)\n",
    "        self._pred = nn.Linear(N_EMBED, dataset.n_vocab)\n",
    "        self._loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        emb = self._emb(batch.ctx)\n",
    "        hid = emb.sum(dim=1)\n",
    "        logits = self._pred(hid)\n",
    "        loss = self._loss(logits, batch.tgt)\n",
    "        return loss\n",
    "    \n",
    "    def represent(self, indices):\n",
    "        return self._pred.weight[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 'epoch'\n",
    "TRN_LOSS = 'trn loss'\n",
    "ISOM = 'isom'\n",
    "HOM = 'hom'\n",
    "LOG_KEYS = [EPOCH, TRN_LOSS, ISOM,  HOM]\n",
    "LOG_FMTS = ['d',   '.3f',    '.3f', '.3f']\n",
    "        \n",
    "def validate(dataset, model, logger):\n",
    "    comp_batch = dataset.get_comp_batch()\n",
    "    reps_uni1 = unwrap(model.represent(comp_batch.uni1))\n",
    "    reps_uni2 = unwrap(model.represent(comp_batch.uni2))\n",
    "    reps_uni = np.concatenate((reps_uni1, reps_uni2))\n",
    "    reps_bi = unwrap(model.represent(comp_batch.bi))\n",
    "    exprs_uni = comp_batch.uni1 + comp_batch.uni2\n",
    "    exprs_bi = list(zip(comp_batch.uni1, comp_batch.uni2))\n",
    "    comp = evals.comp_eval(reps_uni, exprs_uni, reps_bi, exprs_bi, lambda x, y: x + y, evals.cos_dist)\n",
    "    cstr = ['%.3f' % n for n in sorted(comp)]\n",
    "    logger.update(HOM, np.mean(comp))\n",
    "\n",
    "def train(dataset, model):\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    sched = opt_sched.ReduceLROnPlateau(opt, factor=0.5, verbose=True, mode='max')\n",
    "    logger = Logger(LOG_KEYS, LOG_FMTS)\n",
    "    logger.begin()\n",
    "    validate(dataset, model, logger)\n",
    "    logger.print()\n",
    "    \n",
    "    for i in range(10):\n",
    "        trn_loss = 0\n",
    "        for j in range(200):\n",
    "            batch = dataset.get_batch(N_BATCH)\n",
    "            loss = model(batch)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            trn_loss += unwrap(loss)[0]\n",
    "        trn_loss /= 100\n",
    "        \n",
    "        logger.update(EPOCH, i)\n",
    "        logger.update(TRN_LOSS, trn_loss)\n",
    "        validate(dataset, model, logger)\n",
    "        #sched.step(val_acc)\n",
    "        logger.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX 1\n",
      "|        epoch |     trn loss |         isom |          hom |\n",
      "|              |              |              |        1.004 |\n",
      "|            0 |       14.575 |              |        0.908 |\n",
      "|            1 |       13.307 |              |        0.884 |\n",
      "|            2 |       12.796 |              |        0.869 |\n",
      "|            3 |       12.534 |              |        0.851 |\n",
      "|            4 |       12.291 |              |        0.840 |\n",
      "|            5 |       12.049 |              |        0.829 |\n",
      "|            6 |       11.891 |              |        0.821 |\n",
      "|            7 |       11.742 |              |        0.812 |\n",
      "|            8 |       11.656 |              |        0.807 |\n",
      "|            9 |       11.521 |              |        0.803 |\n",
      "CTX 3\n",
      "|        epoch |     trn loss |         isom |          hom |\n",
      "|              |              |              |        1.003 |\n",
      "|            0 |       16.715 |              |        0.915 |\n",
      "|            1 |       14.676 |              |        0.862 |\n",
      "|            2 |       13.976 |              |        0.822 |\n",
      "|            3 |       13.368 |              |        0.797 |\n",
      "|            4 |       12.960 |              |        0.771 |\n",
      "|            5 |       12.583 |              |        0.750 |\n",
      "|            6 |       12.192 |              |        0.737 |\n",
      "|            7 |       11.926 |              |        0.723 |\n",
      "|            8 |       11.676 |              |        0.708 |\n",
      "|            9 |       11.545 |              |        0.699 |\n",
      "CTX 5\n",
      "|        epoch |     trn loss |         isom |          hom |\n",
      "|              |              |              |        0.999 |\n",
      "|            0 |       19.257 |              |        0.940 |\n",
      "|            1 |       16.490 |              |        0.884 |\n",
      "|            2 |       15.498 |              |        0.840 |\n",
      "|            3 |       14.722 |              |        0.806 |\n",
      "|            4 |       14.120 |              |        0.778 |\n",
      "|            5 |       13.566 |              |        0.756 |\n",
      "|            6 |       13.128 |              |        0.738 |\n",
      "|            7 |       12.722 |              |        0.723 |\n",
      "|            8 |       12.412 |              |        0.705 |\n",
      "|            9 |       12.174 |              |        0.694 |\n",
      "CTX 7\n",
      "|        epoch |     trn loss |         isom |          hom |\n",
      "|              |              |              |        1.002 |\n",
      "|            0 |       22.200 |              |        0.961 |\n",
      "|            1 |       18.688 |              |        0.905 |\n",
      "|            2 |       17.483 |              |        0.861 |\n",
      "|            3 |       16.458 |              |        0.828 |\n",
      "|            4 |       15.635 |              |        0.798 |\n",
      "|            5 |       14.986 |              |        0.775 |\n",
      "|            6 |       14.394 |              |        0.757 |\n",
      "|            7 |       13.904 |              |        0.738 |\n",
      "|            8 |       13.463 |              |        0.724 |\n",
      "|            9 |       13.085 |              |        0.707 |\n"
     ]
    }
   ],
   "source": [
    "for ctx in [1, 3, 5, 7]:\n",
    "    print('CTX %d' % ctx)\n",
    "    dataset = Dataset(ctx)\n",
    "    model = Model(dataset)\n",
    "    train(dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
